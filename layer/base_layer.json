{
    "Dense": {
        "Pre Attn Norm": "layernorm (RMSNorm)",
        "Attention block": [
            "query down proj",
            "layernorm (RMSNorm)",
            "query up proj",
            "kv down proj",
            "layernorm (RMSNorm)",
            "kv up proj",
            "rotary emb (RoPE)",
            "score layer",
            "mask + scale + softmax",
            "context_layer",
            "dense_layer (Out proj)",
            "residual addition"
        ],
        "Post-Attn Norm": "layernorm (RMSNorm)",
        "FeedForward block": [
            "dense_layer (Gate Proj)",
            "dense_layer (Up proj)",
            "activation (SiLU)",
            "dense_layer (Down Proj)",
            "residual addition"
        ]
    },
    "MoE": {
        "Pre Attn Norm": "layernorm (RMSNorm)",
        "Attention block": [
            "query down proj",
            "layernorm (RMSNorm)",
            "query up proj",
            "kv down proj",
            "layernorm (RMSNorm)",
            "kv up proj",
            "rotary emb (RoPE)",
            "score layer",
            "mask + scale + softmax",
            "context_layer",
            "dense_layer (Out proj)",
            "residual addition"
        ],
        "Post-Attn Norm": "layernorm (RMSNorm)",
        "FeedForward block": {
            "Gate": [
                "dense_layer (Gate Proj, Routed Expert)",
                "dense_layer (Up proj, Routed Expert)",
                "activation (SiLU)",
                "dense_layer (Down Proj, Routed Expert)",
                "dense_layer (Gate Proj, Shared Expert)",
                "dense_layer (Up proj, Shared Expert)",
                "activation (SiLU)",
                "dense_layer (Down Proj, Shared Expert)",
                "residual addition"
            ]
        }
    }
}
