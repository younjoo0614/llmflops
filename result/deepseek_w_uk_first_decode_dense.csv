Layer Name,FLOPS,InputA,InputB,Output
pre_attn_norm,29360128,"1,7168,1",,"1,7168,1"
query_down,22548578304,"1,7168,1","7168,1536,1","1,1536,1"
k_rope_w,939524096,"1,7168,1","7168,64,1","64,2048,1"
kv_down,7516192768,"1,7168,1","7168,512,1","2048,512,1"
norm_for_compressed_q,6291456,"1,1536,1",,"1,1536,1"
norm_for_compressed_kv,2097152,"2048,512,1",,"2048,512,1"
q_rope_w,25769803776,"1,1536,1","1536,8192,1","128,64,1"
query_up,51539607552,"1,1536,1","1536,16384,1","1,16384,1"
transposed (k up proj),17179869184,"1,16384,1","512,16384,1","1,512,128"
q_rope,50331648,"128,64,1",,"128,64,1"
k_rope,393216,"64,2048,1",,"64,2048,1"
score layer for RoPE,25778192384,"128,64,1","64,2048,1","128,2048,1"
score layer for NoPE,206225539072,"1,512,128","2048,512,1","1,2048,128"
mask_scale_softmax,1409744896,"128,2048,1",,"128,2048,1"
context_matmul,206225539072,"128,2048,1","2048,512,1","1,512,128"
v_up_proj_context,17179869184,"1,512,128","512,16384,1","1,128,128"
out_proj_context,240518168576,"1,128,128","16384,7168,1","1,7168,1"
residual_addition,7340032,"1,7168,1",,"1,7168,1"
post_attn_norm,29360128,"1,7168,1",,"1,7168,1"
gate_proj,270582939648,"1,7168,1","7168,18432,1","1,18432,1"
up_proj,270582939648,"1,7168,1","7168,18432,1","1,18432,1"
silu,132120576,"1,18432,1",,"1,18432,1"
down_proj,270582939648,"1,18432,1","18432,7168,1","1,7168,1"
residual_addition2,7340032,"1,7168,1",,"1,7168,1"
Total FLOPS,1634844082176,,,
